# digm5010
DM Foundations

## Literature Review - Part 2
Bibliography

Anderson, Christine. “Dynamic Networks of Sonic Interactions: An Interview with Agostino Di Scipio.” Computer Music Journal 29, no. 3 (2005): 11–28. https://doi.org/10.1162/0148926054798142. 

Bahn, Curtis, Tomie Hahn, and Dan Trueman. “Physicality and Feedback: a Focus on the Body in the Performance of Electronic Music.” Proceedings of the International Computer Music Conference, 2001, 44–51.

Berdahl, Edgar Joseph. “Applications of Feedback Control to Musical Instrument Design,” 2009. 

Bowers, John, and Sten Olof Hellström. “Simple Interfaces to Complex Sound in Improvised Music.” CHI '00 extended abstracts on Human factors in computing systems  - CHI '00, 2000. https://doi.org/10.1145/633292.633364. 

Bown, Oliver, Alice Eldridge, and Jon Mccormack. “Understanding Interaction in Contemporary Digital Music: from Instruments to Behavioural Objects.” Organised Sound 14, no. 2 (2009): 188–96. https://doi.org/10.1017/s1355771809000296. 

Camurri, A., G. Volpe, G. De Poli, and M. Leman. “Communicating Expressiveness and Affect in Multimodal Interactive Systems.” IEEE Multimedia 12, no. 1 (2005): 43–53. https://doi.org/10.1109/mmul.2005.2. 

Clarke, Eric F. Ways of Listening an Ecological Approach to the Perception of Musical Meaning. Oxford: Oxford University Press, 2005. 

Collins, N. M. “Towards Autonomous Agents for Live Computer Music: Realtime Machine Listening and Interactive Music Systems,” 2006. 

Cypess, Rebecca, and Steven Kemper. “The Anthropomorphic Analogy: Humanising Musical Machines in the Early Modern and Contemporary Eras.” Organised Sound 23, no. 2 (2018): 167–80. https://doi.org/10.1017/s1355771818000043. 

Donnarumma, Marco. “Music for the Flesh II: Informing Interactive Music Performances with the Viscerality of the Body System.” NIME, 2012. 

Driscoll, John, and Matt Rogalsky. “David Tudor's Rainforest: An Evolving Exploration of Resonance.” Leonardo Music Journal 14 (2004): 25–30. https://doi.org/10.1162/0961121043067415.

Eck, Cathy van. Between Air and Electricity: Microphones and Loudspeakers as Musical Instruments. New York, NY: Bloomsbury Academic, 2018. 

Godøy, Rolf Inge., and Marc Leman. Musical Gestures Sound, Movement, and Meaning. New York: Routledge, 2010. 

Hegarty, Paul. “Noise Music.” The Semiotic Review of Books, 2006, 1–6. 

Kahn, Douglas. Noise, Water, Meat: a History of Sound in the Arts. Cambridge, MA: MIT Press, 1999. 

Kemper, Steven, and Rebecca Cypess. “Can Musical Machines Be Expressive? Views from the Enlightenment and Today.” Leonardo 52, no. 5 (2019): 448–54. https://doi.org/10.1162/leon_a_01477. 

Knight, Arthur, and Simon Frith. “Performing Rites: On the Value of Popular Music.” American Music 16, no. 4 (1998): 485. https://doi.org/10.2307/3052293. 

LaBelle, Brandon. Sonic Agency: Sound and Emergent Forms of Resistance. Cambridge, MA: MIT Press, 2020. 

Landy, Leigh. Understanding the Art of Sound Organization. Cambridge, MA: MIT Press, 2007. 

Leman, Marc. Embodied Music Cognition and Mediation Technology. Cambridge, MA: MIT Press, 2008. 

Lähdeoja, Otso. “Composing the Context: Considerations on Materially Mediated Electronic Musicianship.” Organised Sound 23, no. 1 (2017): 61–70. https://doi.org/10.1017/s1355771817000280. 

Maes, Pieter-Jan, Marc Leman, Caroline Palmer, and Marcelo M. Wanderley. “Action-Based Effects on Music Perception.” Frontiers in Psychology 4 (January 2014). https://doi.org/10.3389/fpsyg.2013.01008.

Magnusson, Thor. “Designing Constraints: Composing and Performing with Digital Musical Systems.” Computer Music Journal 34, no. 4 (2010): 62–73. https://doi.org/10.1162/comj_a_00026. 

Oliveros, Pauline. Software for People: Collected Writings 1963-80. Kingston, NY: Pauline Oliveros Publications, 2015.

Overholt, Dan, Edgar Berdahl, and Robert Hamilton. “Advancements in Actuated Musical Instruments.” Organised Sound 16, no. 2 (2011): 154–65. https://doi.org/10.1017/s1355771811000100.

Raphael, Christopher. “Music Plus One and Machine Learning.” ICML 2010 - Proceedings, 27th International Conference on Machine Learning, 2010, 21–28.

Reybrouck, Mark, and Elvira Brattico. “Neuroplasticity beyond Sounds: Neural Adaptations Following Long-Term Musical Aesthetic Experiences.” Brain Sciences 5, no. 1 (2015): 69–91. https://doi.org/10.3390/brainsci5010069.

Rowe, Robert. Interactive Music Systems: Machine Listening and Composing. Cambridge, MA: MIT Press, 1994.

Samuels, Koichi, and Franziska Schroeder. “Performance without Barriers: Improvising with Inclusive and Accessible Digital Musical Instruments.” Contemporary Music Review 38, no. 5 (2019): 476–89. https://doi.org/10.1080/07494467.2019.1684061.

Schiavio, Andrea, and Simon Høffding. “Playing Together without Communicating? A Pre-Reflective and Enactive Account of Joint Musical Performance.” Musicae Scientiae 19, no. 4 (2015): 366–88. https://doi.org/10.1177/1029864915593333.

Scipio, Agostino Di. “Dwelling in a Field of Sonic Relationships.” Live Electronic Music, 2017, 17–45. https://doi.org/10.4324/9781315776989-2.

Scipio, Agostino Di. “‘Sound Is the Interface’: from Interactive to Ecosystemic Signal Processing.” Organised Sound 8, no. 3 (2003): 269–77. https://doi.org/10.1017/s1355771803000244.

Sterne, Jonathan. “Music as a Media Problem: Some Comments and Approaches.” Repercussions, 2014. http://www.ocf.berkeley.edu/~repercus/wpcontent/uploads/2015/1/repercussions-Vol-11–Sterne-Jonathan-Music-as-aMedia-Problem.pdf.

Tanaka, Atau. “Sensor-Based Musical Instruments and Interactive Music.” Essay. In The Oxford Handbook of Computer Music. Oxford: Oxford University Press, 2011.

Truax, Barry. “Electroacoustic Music and the Soundscape: The Inner and Outer World.” Essay. In Companion to Contemporary Musical Thought, 374–98. London: Routledge, 1992.

Voskuhl, Adelheid. Androids in the Enlightenment: Mechanics, Artisans, and Cultures of the Self. Chicago, IL: The University of Chicago Press, 2015.

Wanderley, M.m., and P. Depalle. “Gestural Control of Sound Synthesis.” Proceedings of the IEEE 92, no. 4 (2004): 632–44. https://doi.org/10.1109/jproc.2004.825882.

Waters, Simon. “Performance Ecosystems: Ecological Approaches to Musical Interaction.” EMS: Electroacoustic Music Studies Network , 2007, 1–20.

Wessel, David, and Matthew Wright. “Problems and Prospects for Intimate Musical Control of Computers.” Computer Music Journal 26, no. 3 (2002): 11–22. https://doi.org/10.1162/014892602320582945.

Xia, Guangyu. “Expressive Collaborative Music Performance via Machine Learning,” 2016.



## Journal 1

### Sketch #1: GrainSOM

Looping is an important of my practice - the trajectory of my instrumentation included a path from guitar -> guitar with pedals -> guitar with laptop processing -> computer based instrumentation. Throughout that chain, looping has been a fundamental part. With it has come the challenge of using and interacting with loops that allows them to move beyond static repetitions with maybe variations in time or pitch - how can I capture some sonic input and have dynamically transform and react over time? Some of my recent performance systems have included looping control gestures - draw some sort of shape or path with a stylus, map it out to some synthesis process, and then loop that control data. Seeing the initial Yellowtail sketch, I instantly imagined each mark as a control gesture, and it’s movement in space a possible way for having it evolve over time.


<p class="codepen" data-height="377" data-theme-id="dark" data-default-tab="js,result" data-user="kieranmaraj" data-slug-hash="YzqvMPw" data-preview="true" style="height: 377px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;" data-pen-title="GrainSOM">
  <span>See the Pen <a href="https://codepen.io/kieranmaraj/pen/YzqvMPw">
  GrainSOM</a> by Kieran Maraj (<a href="https://codepen.io/kieranmaraj">@kieranmaraj</a>)
  on <a href="https://codepen.io">CodePen</a>.</span>
</p>
<script async src="https://static.codepen.io/assets/embed/ei.js"></script>



Trying to implement this also allowed me to start exploring something that’s been on my to-do list for awhile now: audio programming outside of a visual programming environment. I used the tone.js library as a starting point, but wasn’t particularly impressed with their default granular algorithm. I ended up basing my own implementation on the one at gloo.xyz. It feels like a vastly different way of thinking about audio than in Max or pd. One difference was a clear lack of differentiation between float values and signals, though I imagine this might just be own lack of familiarity with the API. A bigger difference, however, was in the interaction with scheduling events. Starting grains and envelopes seems to always involve taking the time at a given moment, as well as calculating when it will end. I feel like there’s no clear analogy to this in Max - I wonder is this kind of thing happening under the hood and completely extracted away? Overall it was a fun little challenge to sink into. I’ve become more interested in DSP programming over time, but don’t have the strongest math background and always find approaching this world intimidating and challenging.
	While I’m satisfied with this sketch as a first attempt, there are a number of things I would change on a second iteration. The main thing I would focus on is that the motion of each mark doesn’t clearly translate into what is heard in the synthesis. In its current state, I use a SOM trained on a set of random vectors to set synthesis parameters - as a mark traverses the screen, the head of the mark samples from the SOM and updates the synthesis parameters. I thought this might be an interesting way of creating variation and unpredictability, but also allow for a smoother morphing through synthesis states. However, I think the end result feels too decoupled from the marks motion.
	A few things I would do on a second iteration:
- A mark’s x position would set it’s grain’s panning position
- A mark’s y position would set the grain’s size
- Distance from centre of the screen sets sample position
- A mark’s change in velocity modulates it’s pitch - a sort of doppler effect
- A mark’s length controls the reverb mix for a grain - longer mark resulting in more reverb, or perhaps the reverse
- A mark’s alpha sets it’s grain’s volume
- The SOM would instead modulate synthesis parameters instead of directly setting those parameters

### No Input Mixing

The prompt to reflect on no input mixing was a nice excuse to revisit the practice, as its been a year or two since I last worried I might push my monitors too hard :)

Part of the magic in no input mixing is that it feels like this dynamic, unpredictable beast, but as you spend time with it there are general mappings or shapes of mappings that are learnable, or at least knowable in the sense that in you can know that with *this set of* connections, turning up *these* sends will make *those* sliders effect *this* thing about the sound. But there’s always subtle variation. Every slight turn of a knob or push of a fader results in some connected change that you then have to react to as a performer. In a way it becomes a duet between human and machine. The mixer provides a sort of access point into a dense sonic world that must then be sculpted and shaped, and that shaping in turn effects and transforms that sonic world.

The complexity and surprises of the system, while wonderful and exciting, present an interesting challenge, though. While it's a joy to play solo, and undoubtedly works well in improvised and/or noise contexts, playing no input mixer in anything even approaching a "traditional" ensemble is difficult. The sonic aesthetics of the system are one aspect of why this might be, but I think it's actually the unpredictability and reactivity of the instrument that present a bigger challenge. I can certainly imagine NIM squeals and jitters working in a rock context, or its overwhelming bass working in dance music - but perhaps only if performing those things is repeatable? 

This leads me towards an a potential area of research. I subscribe to Christopher Small's definition of 'music' as being socially constructed - that music is not the score, it's not the vinyl record, it's not the encoded mp3 file, or even the human perception of modulated air. Small defines music as being the set of social relations that come together to make up a musical performance - everyone from the performers themselves to the audience, sound engineer to custodial staff, all are involved to varying degrees in the act of musicking. So my question then is how does the involvement of machines, particularly 'intelligent' machines (whatever that might mean), complicate that set of relations? What does it mean if a machinic instrument can only be performed with other machines, or only in particular ways, to the exclusion of human performers?

## Literature Review - Part 1

I have to admit, the suggestion to project into the future an imagine what a field might look like has thrown me for a bit of a loop. Projections of the future look increasingly apocalyptic. I find myself asking questions about the purpose of music, performance, art in a climate apocalypse - and these questions are actually, I think, a little easier to grapple with. There is always value in art and expression, and comfort in times of strife. But, more specifically, what is the role of technologically mediated art in that environment? And, thinking of Small's definition above, who gets to be involved? Do technology based music become reserved for an affluent few? Are there ways that digital instruments/music making/performing exist to benefit a wider swath of people? Do these practices take on new roles? I don't know how to answer or really even engage with these questions, but they were in mind throughout my research process.

Some keywords:
- Digital music instruments
- Gestural interfaces
- Machine learning
- Computer music
- Embodiment and embodied cognition
- Interactive music systems
- Sound art
- Environmental sound
- Expanded listening
- Machine listening

On the last point, machine listening, I found something unexpected. In looking into machine listening, I was expecting to find work dealing with machine analysis and interpretation of sound, spectral analysis, auditory scene analysis, and so on. What was surprising was finding an emerging use of the term to refer to devices that listen and observe - smart phones that record audio surreptitiously, Amazon Alexas and Google Homes, smart TVs and IoT connected appliances - related to present issues around surveillance, privacy, control, and manipulation. This is another area that I'm very interested in - and that I do think is related in issues around machine learning and big data - but have struggled to figure out how to engage with creatively, or to synthesize with my sound and music interests. The apparent synchronicity of stumbling upon this new use of "machine listening" has me wondering if there is an interesting way to think about surveillance, sound performance, and musical systems - but maybe I'm going to broad when I should be narrowing in.



