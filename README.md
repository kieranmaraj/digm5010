# digm5010
DM Foundations

## Video Tutorial

[![IMAGE ALT TEXT](http://img.youtube.com/vi/NrkohuvRBGo/0.jpg)](http://www.youtube.com/watch?v=NrkohuvRBGo "Video Title")

## Literature Review

### Keywords
Electroacoustic music, digital musical instruments, feedback systems, machine listening

### Research Question

I have to admit, the suggestion to project into the future an imagine what a field might look like has thrown me for a bit of a loop. Projections of the future look increasingly apocalyptic. I find myself asking questions about the purpose of music, performance, art in a climate apocalypse - and these questions are actually, I think, a little easier to grapple with. There is always value in art and expression, and comfort in times of strife. But, more specifically, what is the role of technologically mediated art in that environment? And, thinking of Small's definition above, who gets to be involved? Do technology based music become reserved for an affluent few? Are there ways that digital instruments/music making/performing exist to benefit a wider swath of people? Do these practices take on new roles? I don't know how to answer or really even engage with these questions, but they were in mind throughout my research process.

Some keywords:
- Digital music instruments
- Gestural interfaces
- Machine learning
- Computer music
- Embodiment and embodied cognition
- Interactive music systems
- Sound art
- Environmental sound
- Expanded listening
- Machine listening

On the last point, machine listening, I found something unexpected. In looking into machine listening, I was expecting to find work dealing with machine analysis and interpretation of sound, spectral analysis, auditory scene analysis, and so on. What was surprising was finding an emerging use of the term to refer to devices that listen and observe - smart phones that record audio surreptitiously, Amazon Alexas and Google Homes, smart TVs and IoT connected appliances - related to present issues around surveillance, privacy, control, and manipulation. This is another area that I'm very interested in - and that I do think is related in issues around machine learning and big data - but have struggled to figure out how to engage with creatively, or to synthesize with my sound and music interests. The apparent synchronicity of stumbling upon this new use of "machine listening" has me wondering if there is an interesting way to think about surveillance, sound performance, and musical systems - but maybe I'm going to broad when I should be narrowing in.

### Initial Reading List

Anderson, Christine. “Dynamic Networks of Sonic Interactions: An Interview with Agostino Di Scipio.” Computer Music Journal 29, no. 3 (2005): 11–28. https://doi.org/10.1162/0148926054798142. 

Bahn, Curtis, Tomie Hahn, and Dan Trueman. “Physicality and Feedback: a Focus on the Body in the Performance of Electronic Music.” Proceedings of the International Computer Music Conference, 2001, 44–51.

Berdahl, Edgar Joseph. “Applications of Feedback Control to Musical Instrument Design,” 2009. 

Bowers, John, and Sten Olof Hellström. “Simple Interfaces to Complex Sound in Improvised Music.” CHI '00 extended abstracts on Human factors in computing systems  - CHI '00, 2000. https://doi.org/10.1145/633292.633364. 

Bown, Oliver, Alice Eldridge, and Jon Mccormack. “Understanding Interaction in Contemporary Digital Music: from Instruments to Behavioural Objects.” Organised Sound 14, no. 2 (2009): 188–96. https://doi.org/10.1017/s1355771809000296. 

Camurri, A., G. Volpe, G. De Poli, and M. Leman. “Communicating Expressiveness and Affect in Multimodal Interactive Systems.” IEEE Multimedia 12, no. 1 (2005): 43–53. https://doi.org/10.1109/mmul.2005.2. 

Clarke, Eric F. Ways of Listening an Ecological Approach to the Perception of Musical Meaning. Oxford: Oxford University Press, 2005. 

Collins, N. M. “Towards Autonomous Agents for Live Computer Music: Realtime Machine Listening and Interactive Music Systems,” 2006. 

Cypess, Rebecca, and Steven Kemper. “The Anthropomorphic Analogy: Humanising Musical Machines in the Early Modern and Contemporary Eras.” Organised Sound 23, no. 2 (2018): 167–80. https://doi.org/10.1017/s1355771818000043. 

Donnarumma, Marco. “Music for the Flesh II: Informing Interactive Music Performances with the Viscerality of the Body System.” NIME, 2012. 

Driscoll, John, and Matt Rogalsky. “David Tudor's Rainforest: An Evolving Exploration of Resonance.” Leonardo Music Journal 14 (2004): 25–30. https://doi.org/10.1162/0961121043067415.

Eck, Cathy van. Between Air and Electricity: Microphones and Loudspeakers as Musical Instruments. New York, NY: Bloomsbury Academic, 2018. 

Godøy, Rolf Inge., and Marc Leman. Musical Gestures Sound, Movement, and Meaning. New York: Routledge, 2010. 

Hegarty, Paul. “Noise Music.” The Semiotic Review of Books, 2006, 1–6. 

Kahn, Douglas. Noise, Water, Meat: a History of Sound in the Arts. Cambridge, MA: MIT Press, 1999. 

Kemper, Steven, and Rebecca Cypess. “Can Musical Machines Be Expressive? Views from the Enlightenment and Today.” Leonardo 52, no. 5 (2019): 448–54. https://doi.org/10.1162/leon_a_01477. 

Knight, Arthur, and Simon Frith. “Performing Rites: On the Value of Popular Music.” American Music 16, no. 4 (1998): 485. https://doi.org/10.2307/3052293. 

LaBelle, Brandon. Sonic Agency: Sound and Emergent Forms of Resistance. Cambridge, MA: MIT Press, 2020. 

Landy, Leigh. Understanding the Art of Sound Organization. Cambridge, MA: MIT Press, 2007. 

Leman, Marc. Embodied Music Cognition and Mediation Technology. Cambridge, MA: MIT Press, 2008. 

Lähdeoja, Otso. “Composing the Context: Considerations on Materially Mediated Electronic Musicianship.” Organised Sound 23, no. 1 (2017): 61–70. https://doi.org/10.1017/s1355771817000280. 

Maes, Pieter-Jan, Marc Leman, Caroline Palmer, and Marcelo M. Wanderley. “Action-Based Effects on Music Perception.” Frontiers in Psychology 4 (January 2014). https://doi.org/10.3389/fpsyg.2013.01008.

Magnusson, Thor. “Designing Constraints: Composing and Performing with Digital Musical Systems.” Computer Music Journal 34, no. 4 (2010): 62–73. https://doi.org/10.1162/comj_a_00026. 

Oliveros, Pauline. Software for People: Collected Writings 1963-80. Kingston, NY: Pauline Oliveros Publications, 2015.

Overholt, Dan, Edgar Berdahl, and Robert Hamilton. “Advancements in Actuated Musical Instruments.” Organised Sound 16, no. 2 (2011): 154–65. https://doi.org/10.1017/s1355771811000100.

Raphael, Christopher. “Music Plus One and Machine Learning.” ICML 2010 - Proceedings, 27th International Conference on Machine Learning, 2010, 21–28.

Reybrouck, Mark, and Elvira Brattico. “Neuroplasticity beyond Sounds: Neural Adaptations Following Long-Term Musical Aesthetic Experiences.” Brain Sciences 5, no. 1 (2015): 69–91. https://doi.org/10.3390/brainsci5010069.

Rowe, Robert. Interactive Music Systems: Machine Listening and Composing. Cambridge, MA: MIT Press, 1994.

Samuels, Koichi, and Franziska Schroeder. “Performance without Barriers: Improvising with Inclusive and Accessible Digital Musical Instruments.” Contemporary Music Review 38, no. 5 (2019): 476–89. https://doi.org/10.1080/07494467.2019.1684061.

Schiavio, Andrea, and Simon Høffding. “Playing Together without Communicating? A Pre-Reflective and Enactive Account of Joint Musical Performance.” Musicae Scientiae 19, no. 4 (2015): 366–88. https://doi.org/10.1177/1029864915593333.

Scipio, Agostino Di. “Dwelling in a Field of Sonic Relationships.” Live Electronic Music, 2017, 17–45. https://doi.org/10.4324/9781315776989-2.

Scipio, Agostino Di. “‘Sound Is the Interface’: from Interactive to Ecosystemic Signal Processing.” Organised Sound 8, no. 3 (2003): 269–77. https://doi.org/10.1017/s1355771803000244.

Sterne, Jonathan. “Music as a Media Problem: Some Comments and Approaches.” Repercussions, 2014. http://www.ocf.berkeley.edu/~repercus/wpcontent/uploads/2015/1/repercussions-Vol-11–Sterne-Jonathan-Music-as-aMedia-Problem.pdf.

Tanaka, Atau. “Sensor-Based Musical Instruments and Interactive Music.” Essay. In The Oxford Handbook of Computer Music. Oxford: Oxford University Press, 2011.

Truax, Barry. “Electroacoustic Music and the Soundscape: The Inner and Outer World.” Essay. In Companion to Contemporary Musical Thought, 374–98. London: Routledge, 1992.

Voskuhl, Adelheid. Androids in the Enlightenment: Mechanics, Artisans, and Cultures of the Self. Chicago, IL: The University of Chicago Press, 2015.

Wanderley, M.m., and P. Depalle. “Gestural Control of Sound Synthesis.” Proceedings of the IEEE 92, no. 4 (2004): 632–44. https://doi.org/10.1109/jproc.2004.825882.

Waters, Simon. “Performance Ecosystems: Ecological Approaches to Musical Interaction.” EMS: Electroacoustic Music Studies Network , 2007, 1–20.

Wessel, David, and Matthew Wright. “Problems and Prospects for Intimate Musical Control of Computers.” Computer Music Journal 26, no. 3 (2002): 11–22. https://doi.org/10.1162/014892602320582945.

Xia, Guangyu. “Expressive Collaborative Music Performance via Machine Learning,” 2016.

### Annotated Bibliography

Cypess, Rebecca, and Steven Kemper. “The Anthropomorphic Analogy: Humanising Musical Machines in the Early Modern and Contemporary Eras.” Organised Sound 23, no. 2 (2018): 167–80. https://doi.org/10.1017/s1355771818000043. 

  In this paper Cypess and Kemper trace a lineage of understanding musical instruments in relation to human bodies. They begin by looking at 17th century Europe, a time period in which the understanding of instruments, musical and otherwise, was in flux. They identify this as a time when a strong connection between human body and instrument begins to emerge, and make a connection to contemporary musical systems and experimental instruments that probe at this relationship.

The authors begin by noting early 17th century Europe as a time and place where the understanding of what tools were **meant for** was changing; where they had previously been a means of reproduction and repetition, they were increasingly becoming a way of understanding the world and methods of discovery. The authors note this changing relationship appearing in the writings of Descartes and other philosophers of the time. They identify the changes as reciprocal - as people began to think of tools and machines as ways of exploring the world, there was also a changing understanding of the human body. They began to understand the body as functioning similarly to tools and machines. And within this web of relationships, the understanding of musical instruments was also changing.

They identify an emerging feature amongst composers - more consideration was given by composer to the instrumentalist and instrument. What was special about, for example, a violin that a clarinet could not do? The instrument was not simply a tool for reproducing the composers intention, but also a way for them to explore what the music could be. At the same time there was an emergent understanding of instruments in relation to the human body. Cypress and Kemper point to writings that consider the bellows of an organ its lungs and the keys it’s teeth. They point to artworks that depict humans merged with instruments, suggesting a blurry line between human and instrument - a suggestion towards considering the instrument as an extension of the body, and not a separate entity. Anecdotally, this is something I personally find very intuitive to understand. I’ve certainly experienced flow state moments when my instrument and my body have felt like a continuous unit, with very little conscious thought going towards “operating” the instrument. This is something I’ve only really experienced with traditional instruments, however, and not digital systems, and am curious if it’s possible to achieve the same with non-traditional instruments.
	
  From there Cypess and Kemper jump to the contemporary era. They point towards a number of music technologies that have continued to examine, in new ways, this relationship between body and instrument. These include performing robots, some of which attempt to mimic the human body and another that attempts to play music that would be impossible for a human. They also look at glove based digital instruments that seek to augment performer’s capabilities - these are the most similar to traditional instruments as they require direct, bodily interaction and intention from the performer. The system that I find most fascinating, however, is a biophysical system in which MIDI data is connected to electrodes on performer bodies, and those electrodes stimulate muscles, controlling a performers movements in playing their instruments. Connecting it Dockray’s thesis, there’s a very clear transfer of control here. I have an almost visceral reaction to this, and don’t think I would necessarily ever want to create something like this, but at the same time I am fascinated by it.

Dockray, Sean. 2019. “Performing Algorithms: Automation and Accident.”

Sean Dockray is an artist and writer whose work examines the politics of technology and the web. His work primarily takes the form of essays and video essays. This work is his thesis, a collection of thematically related essays.

Dockray begins by making note of a common feature in many works of generative art - a complex relationship with the concept of ‘control.’  He notes many artists and works which transfer control from themselves to the machine in their generative works. This is a key feature, if not **the** key feature, of generative work. Human control is given over to algorithmic control. He points to a number of artists who call this a liberating process, a process full of surprise, and on which creates curiosity about choices made by the system. Dockray suggests a connection between algorithmic control and the Deleuzian concept of control societies. He then make a connection to the present day in which much of life is mediated by algorithmic control.
	
  He explicitly states, however, that he is not trying to say that generative art is a bad thing, or even that algorithmic control is a negative thing, but instead suggests that the increasingly complicated relationship with control in modern life is reflected in generative art. He also notes that while he is suggesting connections, pointing towards a sort of Silicon Valley/surveillance capitalist system of control, it’s also important not to give those systems *too much* credit or suggest that control is total. The reality of those systems is likely much more sloppy than that, full of accidents and unexpected results - as is often the case in systems for generative art.
	
  I found this thesis to be absolutely fascinating, though sometimes the philosophically dense sections flew over my head. In designing musical systems, there is always a question of control - how much and what controls should be given to machine and what to human? How do you create a control structure that is satisfying to engage with? It’s certainly making me consider that relationship with control in new ways. As many other pieces I’ve read suggest that musical systems can embody social relationships and imaginings, how can the negotiation of algorithmic control also imagine or embody new social dynamics? 


Herndon, Holly, Mat Dryhurst, and Sean Dockray. 2020. “Inhuman Intelligence: Holly Herndon and Mat Dryhurst in Conversation with Sean Dockray on Disclaimer.” Disclaimer.org.Au. June 2020. https://disclaimer.org.au/contents/holly-herndon-and-mat-dryhurst-in-conversation-with-sean-dockray.

  Holly Herndon is one of my favourite musicians. Her work has had a significant influence on the way I think about the world, and I wanted to include this interview as the discussion really resonates with me and touches on an approach to working with technology that I find appealing. Herndon’s work touches upon social, political, and technological issues around performing and creating with technology, and manages to bring conceptual ideas together in a way that is aesthetically satisfying and coherent. 
	
  In this interview Herndon and partner Mat Dryhurst are interviewed by Sean Dockray. They discuss Herndon and Dryhurst’s most recent project, Spawn. They call Spawn their ‘AI baby’, but clarify that it is really a metaphor for a set of experiments in machine learning and music making. The metaphor informs their approach to working and developing the technology - primarily in thinking about the development process as ‘raising’ Spawn. They attempt to raise Spawn as a member of Herndon’s vocal ensemble, as a member of a community. All the training data in the project is highly personal. Working with audio over MIDI (to preserve information like timbre and context) all of the audio Spawn is trained on comes from Herndon, her vocal ensemble, and audiences at their live performances. None of the training data is anonymous, nor does it come from large data sets, and everyone who individually contributes to that data, to raising the baby, is credited and compensated.
	
  This approach of using highly personal data is one that really appeals to me. It acknowledges the highly collaborative nature of both software development and music creation. Compared to other music and machine learning projects that use vast amounts of data culled from the internet without crediting or compensating the artists who created that work (AI-Jukebox by Open AI is what most clearly comes to mind), this approach feels like it acknowledges the work of all who contribute to it. It brings people in and gives them agency in the process. The anonymous and culled approach harkens back to the early recorded music industry, with a power structure that takes advantage of artists and does not compensate them fairly.  There are also echoes of the issues in sampling culture. At the same time, I appreciate that Dryhurst and Herndon note that a lot of powerful and creative work came from those traditions, but that in itself doesn’t negate the problematic aspects. It is a complex space to work within, and not just because of the technical nature.
	
  Finally, they discuss the role that machine learning systems can play in the act of live performance. In pushing back against the common narrative of AI replacing humans, they instead suggest that machine learning systems can be used as ways of augmenting human relationships and acting as co-ordination systems for human activity. They state that these systems should ‘ambiently witness’ real human activity, and facilitate the conditions for it. With Spawn, they took inspiration of early communal vocal practices - thinking about group singing as co-ordination technology, and trying to extend that into their own system. They attempt to use technology in performance to allow them to be more human. Again, this idea of systems that augment relationships and facilitate creation is one that I find very appealing and resonant.

‌

Lewis, George E. 2000. “Too Many Notes: Computers, Complexity and Culture in Voyager.” Leonardo Music Journal 10 (December): 33–39. https://doi.org/10.1162/096112100570585.

Voyager is a composition and interactive music system created by George Lewis. In Too Many Notes Lewis describes the technical implementation of the system, as well as the socio-cultural influences and ideals that informed his choices in building it. Lewis begins by relating various issues faced by Black American and African diasporic artists in Western arts institutions. He identifies a tension between the values (aesthetic, social, cultural, etc) of these institutions, and the values represented in works by Black artists. A characteristic he notes as common and important among works of many Black artists is ‘multidominance’; this refers to a tendency to have a multitude of vibrant, overlapping colours and dense, repeating patterns - a sort of visual maximalism with embedded rhythmic qualities. 
	
  Importantly, Lewis identifies multidominance as a feature that occurs sonically within his work as a member of the Association for the Advancement of Creative Musicians, as well as his work in the worlds of jazz and avant grade music. The social values inherited from a multidominant form of playing - non-hierarchical, improvised, having many voices at once, at times cacophonous and free, and other times coming together in harmony - inform his design decisions in creating the Voyager system. 
	
  Lewis describes Voyager as being not an instrument, but as “multiple parallel streams of music generation, emanating from both the computers and the humans - a non-hierarchical, improvisational subject-subject model of discourse, rather than a stimulus/response setup.” He seems to think of it as something closer to an interactive composition. Voyager ‘listens’ to MIDI input - whether through a MIDI keyboard, or by attempting to convert real time sound input to MIDI. The system extracts information related to pitch, timing, amplitude, and density of events. This information is used to inform the output of the system, which consists of multiple “orchestral performers”, of which various subsets can be activated at different times, can move in and out of metric synchronicity with human performers, and can contrast or harmonize pitch sets. Similarly to the AESI system, Lewis states that all interaction takes place sonically, with the performer having no direct control of the system - it has no controls like footswitches, dials etc. (Though I would personally argue that MIDI keyboard input isn’t exactly a sonic interaction, but perhaps a heavily abstracted one).


Magnusson, Thor. “Designing Constraints: Composing and Performing with Digital Musical Systems.” Computer Music Journal 34, no. 4 (2010): 62–73. https://doi.org/10.1162/comj_a_00026. 

  In this paper Magnusson examines the process of creating, composing for, and performing with digital music systems from a human-computer interaction point of view. He specifically draws upon the HCI concepts of affordances, constraints, and mappings. He ultimately declares that, between affordances and constraints, constraints provide a more valuable way of understanding how musicians interact with digital systems. He states that designers of these systems should approach creating them with the design of constraints as a primary concern.

While there are a number of ways of conceptualizing affordances, Magnusson goes with a non-subjective understanding: the “potential applications derived from the agent’s embodied relationship with the object.” What a performer can physically do with their body to engage the instrument.

Constraints are presented as the opposite side of the same coin. They are the limits of a system that create a structure or map of possibilities. Magnusson even says that all musical systems (theoretical, technological, or otherwise) are all systems of constraints. Magnusson presents his own set of constraints: 

  * Subjective: Expressive limitations faced by the performer
  * Objective: The physical limitations of a system or its materials
  * Cultural: The conditions in which the technologies and ideas co-exist

	Next, Magnusson identifies three components as being common to all digital musical instruments: the physical interface (where affordances lie), the sound producing engine, and the mapping engine. The physical interface and the sound engine are decoupled from one another, and only connected through the mapping engine. Magnusson refers to the mapping and sound engines as the systems ‘body’. Interestingly, understanding of instrument and the digital is facilitated by relation to the human body. I find this interesting as I think of the body as the most physical part of a human - but here Magnusson identifies the most ephemeral part of the system as the body. He says that the process of composing mappings, of **creating constraints**, is what gives a digital instrument its identity. He then examines different digital instruments and notes in the instrument with the most clear physical affordances that any similar controller could be swapped in, and as long as the mapping and sound engine remained the same, the instrument would be functionally the same instrument. There’s an interesting implication here that the physical is ephemeral, and a suggestion that physical interfaces in digital instruments matter less than they do in acoustic instruments. Does the human body also then matter less in working in digital and virtual spaces, or is there something to be gained by fleshing out the digital’s relationship with the body. 

Sanfilippo, Dario, and Andrea Valle. 2013. “Feedback Systems: An Analytical Framework.” Computer Music Journal 37 (2): 12–27. https://doi.org/10.1162/comj_a_00176.


In this paper Sanfiippo, a student of  Di Scipio, traces a history of feedback systems and their role in musical and sonic creation. A feedback system is one in which the output of a system is fed back into its input and experiences some process of transformation. He analyses them for their common behaviours, and proposes a framework for categorizing them. Sanfiippo points to cybernetics and systems theory as being fields in which theories of a feedback systems emerged and from where electronic musicians would take inspiration for creating new sonic feedback systems for musical performance.
	
  Sanfilippo notes several key properties of feedback systems (a system can contain any subset of these):

  * Positive feedback, in which output is connected to input, creates instability
  * Negative feedback, in which output diminishes the input, regulates the system
  * Feedback systems are inherently non-linear; sonic elements that seem unrelated in a linear system become intertwined in a non-linear system
  * Multiple elements can become coupled to one another, meaning that small changes in the network of connections in a feedback system can radically change the systems identity

He also notes behaviours common to many systems: a tendency to self organize or self disorganize - or a tendency towards stability or instability. A feedback system can converge towards a point of stasis, or it might rapidly move through wildly different states in a chaotic fashion. Complex patterns emerge at the global level through low level interactions in the system - ‘self organization is parallel and distributed, it takes place through the simultaneous action of all the elements,’ there is no centralized control. As in many of the other entries in this review, it feels to me like this is pointing towards a way of understanding the world, a way of imagining new ways to structure interaction.
	Finally, Sanfilippo proposes 6 criteria for categorizing feedback systems:

  * Digital or Analog encoding of sound
  * Audio or Sub-audio control rate
  * Closed or Open environment (can it listen to/be affected by its sonic environment?)
  * Internal or External triggering (how does the system start producing sound?)
  * Adaptive or Non-adaptive (does the system organize itself? Does it respond to the world?)
  * Human Interaction or Not (can the system produce sound without human intervention?)

I really enjoy working with feedback systems, and it was interesting to read about a multitude of systems others have built. It’s also revealing to consider these fundamental elements and behaviours of feedback systems. There’s an interpretation of feedback systems as systems that are listening to themselves - as systems that have ears. I’m very interested in exploring this further.
	

Scipio, Agostino Di. “‘Sound Is the Interface’: from Interactive to Ecosystemic Signal Processing.” Organised Sound 8, no. 3 (2003): 269–77. https://doi.org/10.1017/s1355771803000244.

  In this paper Di Scipio describes his project, the Audible Eco-Systemic Interface (AESI). He makes an argument for using audio as the source of all control signals in interactive music systems. AESI is a type of feedback system; in an ecosystem that consists of human performers, loudspeakers, microphones, computer software, and  the physical environment, AESI listens to audio at multiple points - internally to the sound the system generates, and externally to the entirety of its acoustic environment via a pair of microphones. The system performs various analyses - different types or aspects of listening - to the sound signals and then uses this data to change the sound generation parameters of the system itself. Di Scipio notes some of the acoustic features such as amplitude, density of events, spectral properties, and early reflection detection. I particularly found the ‘density of events’ feature interesting - it reminds me of Van Foerster stating that amplitude of signals is not the most salient feature, but frequency. Di Scippio notes that, because of this implementation, the system wouldn’t necessarily respond to a single cough in the audience, but if many people started coughing, this would affect the system’s output.
	
  Di Scipio states that this system represents a “shift from creating wanted sounds via interactive means, towards creating wanted interactions having audible traces.” I’m very much intrigued and drawn towards this process that makes use of sound at a fundamental level, and that makes use of metaphors of listening. I also really like that the work inherently becomes site specific - the acoustic properties of whatever space the system is set up within, and the characteristics of the sound system it is connected to will have an impact on the material that it generates. Di Scippio  even mentions having to tune the system for a given room. However, given his previous statement on ‘creating wanted interactions’ , I’m curious about how performer agency manifests in a system like this? For instrumentalists performing with a system like this, does it end up feeling purely like ‘guiding’ the system to a desired place, and is that a satisfying endeavour?

Truax, Barry. “Electroacoustic Music and the Soundscape: The Inner and Outer World.” Essay. In Companion to Contemporary Musical Thought, 374–98. London: Routledge, 1992.

  I found the this Truax piece to be very interesting as he makes significant reference to Jacques Attali’s Noise - a book on the economic structures of music that has had a large impact on my own thinking. As Attali looks at music as a way of understanding and predicting socio-economic relationships, Truax considers soundscapes as a way of understanding human social relationships and relationship to the natural environment.
	
  Truax first proposes that the soundscape is a model of communication and a way of understanding the world. Information is carried through the environment sonically and must be interpreted by listeners within that environment. This becomes a reciprocal relationship - what we hear in the soundscape influences what we emit out into it. It is a process of shaping, a feedback system. Within the soundscape there are different modes of listening, whether close and focused or general and in the background, but they all contribute to a process of meaning making. Truax considers the pre-industrial soundscape as a sort of ideal - one that has diversity, complexity, and balance. Truax implies that since these qualities were found in the soundscape, they also would have been present in human relationships as well as the human-environment relationship.
	
  With industrialization comes a changing and increasingly complicated soundscape. Truax focuses on electroacoustic reproduction of sound as a major turning point. Electrical recording and reproduction of sound allows sound to have a break with its original context, an important factor in meaning making. This creates contradictions in the interpretation of the soundscape. I do wonder, though, aren’t there new meanings in being able to reproduce any sound?  Truax goes on to state that electroacoustic reproduction has created a media environment, one filled with background music and sonic advertising cues. This new environment has become a surrogate for the natural soundscape. It mirrors social conditions of loneliness, isolation, boredom, and noisiness. The surrogate, filled with constant sound vying for attention, fills this space, but only as a bandage solution. Truax suggests that the complications and contradictions of this soundscape are a reflection of issues in modern society - power imbalances, racism, sexism, etc. 
	
  Truax looks to his own practice as an electroacoustic composer as a potential source of restructuring the soundscape to solve those ills. He works from Attali’s idea that changes in musical structuring and production presage wider social changes. He points to his use of granular composition, and how the atomic elements of his practice mirror values found in the pre-industrial soundscape - variety, complexity, and balance. Importantly, he points to working with computer composition as fundamentally changing the way he thinks about composing - it leads not just to new sounds, but new ways of structuring and thinking about the world. This made me think of the Anothropomorphic Analogy paper - as tools and instruments became ways for exploring and thinking about the world, the computer becomes part of that lineage in Truax’s thinking. It also points to a questioning of the relationship between computer and human body.
	
  Truax also points to a relationship of control with the computer, control again frequently coming up in my literature review. He points to having a shepherd like relation with the computer - one where the computer makes many of the choices in composing, but the human must guide it to a desirable place. This sounds similar to the process of working with AESI. This work was published in 1992, and I wonder if Truax’s thoughts on control and this element of the human-computer relationship has changed?

### Findings

One element I found that appeared consistently was an implied understanding of instruments via the body. The Anthropomorphic Analogy paper **does** make this connection explicit, but in many of the other works, the same thread exists, even though it is not the focus. I also noticed a lack of this instrument-body understanding in the works that focused on feedback systems. I wonder if there is a way to make connections between body and feedback system as instrument - is there some unexplored understanding there? The relationship between body, instrument, and computer is one that I'm very interested in exploring further. 

Additionally, another consistent thread, though not always explicit, seems to be viewing the construction of interactive music systems and the relationships and values embedded in those interactions as ways of imagining the world and constructing new social relationships. To this end, I think constructing systems that facilitate collaboration and that can augment the music making experience is another angle that I find very appealing.


## Journal 3

### Sketch #3: Vehicles Alterations & Process 8

I've expanded on the vehicles sketch from class in a few ways. First, I've modified the code to use classes - I like working this way because it allows me to compartmentalize aspects of the program. With this implementation, it becomes simpler to modify vehicles and to create different behaviours. I can create a number of vehicles and randomly assign each one a behaviour from a predefined set. From there I decided to use the vehicles as drivers of another layer of animation - they provided a sort of underlying environment, an invisible merkwelt, to drive the visible animation. I took inspiration from Process 8 of [Casey Reas's Process Compendium](http://www3.reas.com/text). 

Process 8 is described: 

*A rectangular surface densely filled with instances
of Element 2, each with a different size, speed, and
direction. Display the intersections by drawing a
circle at each point of contact. Set the size of each
circle relative to the distance between the centers
of the overlapping Elements. Draw the smallest
possible circle as black and largest as white, with
varying grays representing sizes in between.*

The vehicles, with their varying behaviours, take the place of 'Element 2's in the description. 

<p class="codepen" data-height="265" data-theme-id="light" data-default-tab="js,result" data-user="kieranmaraj" data-slug-hash="wvWwryX" style="height: 265px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;" data-pen-title="Vehicles">
  <span>See the Pen <a href="https://codepen.io/kieranmaraj/pen/wvWwryX">
  Vehicles</a> by Kieran Maraj (<a href="https://codepen.io/kieranmaraj">@kieranmaraj</a>)
  on <a href="https://codepen.io">CodePen</a>.</span>
</p>
<script async src="https://static.codepen.io/assets/embed/ei.js"></script>

![Vehicles](https://github.com/kieranmaraj/digm5010/blob/gh-pages/images/vehicles/vehicles-bw-1.png?raw=true)
![Vehicles](https://github.com/kieranmaraj/digm5010/blob/gh-pages/images/vehicles/vehicles-bw-2.png?raw=true)

I then expanded the sketch from Reas's intial description by having it sample from visual material. I worked with the Flickr API to randomly load an image from Flickr. Instead of setting the colour as a gradient between black and white, each drawn circle samples from the loaded image. I experimented with various behaviours of sampling - choosing the portion of the image completely randomly, as a function of the distance of overlapping vehicles. One behaviour I particularly liked was a random sampling that gets held for the course of a collision - this results in these textural smears that feel almost like paint splatter paintings.

<p class="codepen" data-height="265" data-theme-id="light" data-default-tab="js,result" data-user="kieranmaraj" data-slug-hash="rNLOrNq" data-preview="true" style="height: 265px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;" data-pen-title="Vehicles-Image Sampling">
  <span>See the Pen <a href="https://codepen.io/kieranmaraj/pen/rNLOrNq">
  Vehicles-Image Sampling</a> by Kieran Maraj (<a href="https://codepen.io/kieranmaraj">@kieranmaraj</a>)
  on <a href="https://codepen.io">CodePen</a>.</span>
</p>
<script async src="https://static.codepen.io/assets/embed/ei.js"></script>

![Vehicles](https://github.com/kieranmaraj/digm5010/blob/gh-pages/images/vehicles/vehicles-samp-3.png?raw=true)
![Vehicles](https://github.com/kieranmaraj/digm5010/blob/gh-pages/images/vehicles/vehicles-samp-4.png?raw=true)
![Vehicles](https://github.com/kieranmaraj/digm5010/blob/gh-pages/images/vehicles/vehicles-samp-2.png?raw=true)


## Journal 2

### Sketch #2: Chitin

[Chitin](https://en.wikipedia.org/wiki/Chitin) is the material that composes insect exoskeletons. It is also an updated version of my previous GrainSOM sketch that implements several of changes I had previously noted. In this version, input gestures have more influence on the sonic behaviour - fast movements result in pitch modulations (a pseduo-doppler effect), longer gestures are routed through a reverb engine, and the mark position influences grain size and overlap. As before, there is a SOM underneath the animation. Whereas in the previous version the SOM weights directly set synthesis parameters, this time the SOM weights modulate those values. I find I prefer this balance between human gesture and machine control - this balance provides variety and unpredictable randomness, but it applies it to a gesture that feels like it had more intention to begin with. 

I’ve additionally implemented some random sampling from [freesound.org](http://freesound.org). I’ve setup a bank of keywords to randomly query the Freesound API with on startup, and then randomly sample from the results to set granulation sources. I’ve found that a lot of the sounds returned are foley-esque sounds, which when layered and granulated on top of each other tend towards dense insect soundscapes - hence Chitin.

<p class="codepen" data-height="438" data-theme-id="dark" data-default-tab="js,result" data-user="kieranmaraj" data-slug-hash="rNeQbov" data-preview="true" style="height: 438px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;" data-pen-title="chitin">
  <span>See the Pen <a href="https://codepen.io/kieranmaraj/pen/rNeQbov">
  chitin</a> by Kieran Maraj (<a href="https://codepen.io/kieranmaraj">@kieranmaraj</a>)
  on <a href="https://codepen.io">CodePen</a>.</span>
</p>
<script async src="https://static.codepen.io/assets/embed/ei.js"></script>

### Braitenberg Vehicles

I had read the first few vehicle descriptions several years ago, and it was interesting to come back to it after a period of time in which the way I think has changed quite a bit. From this subset of vehicles, one of the things that seems unaccounted for is self awareness - Braitenberg  only posits that the vehicles *appear* to be aware to an outside observer. I’m curious if the later vehicles find ways to take this into account. 

The idea that intelligent beings are simply composed of a vast array of simple sensory inputs and outputs is one that I’ve encountered frequently, and one that I have mixed feelings about. I think that at a material, biological level it is true, and it opens up exciting new possibilities for how we think about knowledge and intelligence and who and what we consider to be intelligent. It provides a framework for, perhaps, approaching the intelligence of plants and living things that are vastly different from humans. But at the same time, the implication that humans - and other life - are advanced machinery intersects with the socio-economic aspects of the human world in unfortunate ways. If humans are machines, and we look at the history of machines and, say, their relationships with labour, what does it mean if we treat humans like machines? Does the power dynamic of that become one in which people are expected to do and perform to exact specifications, like a computer? And, tangential but related, I find it interesting how our metaphorical conceptions of the mind have changed over time, and how they seem to reflect technological trends of their time: the humors of ancient Greece, to factories of the industrial revolution, to the computers of now.

Regarding the more technical systems design material, though, I’m less conflicted and mostly just excited about the possibilities of applying some of these concepts in sound systems. I found myself thinking again about no-input mixing and how many of the feedback loops, gates and thresholds Braitenberg discusses feel like qualities inherent in a no-input mixer - thresholds for big sonic leaps, a dynamic variation of those thresholds, complex behaviour emerging from simple mappings, and behaviour at the output directly feeding back into the system. 

### Interface as Instrument

This piece really resonated with me, as well as gave me a bit of a laugh. It was published in 2005, which would have been right around the time I first started playing with digital audio with a copy of Cool Edit Pro, and I find it so funny that what Magnusson says about digital instruments not needing to be so indebted to analog studio technology is a major frustration I’ve had with the world of DAWs and plugins, 15 years later. It’s definitely gotten better in the last few years, I think in part due to the popularity of modular synthesis, but it certainly feels like there’s still vast room for experimentation, even in the world of commercial software. I’ve personally often found weird, free plugins that only work half the time to be the most interesting sound generators. 

The discussion of interface is interesting, but I find myself less interested in the onscreen interface than the physical interface into sonic systems. I worked as a recording engineer for a few years, and really developed this sense that working with digital audio, even at just the level of eq’ing and compressing is a process akin to sculpting. While I was spending most of my time editing recordings of bands inside ProTools and ideas of digital instrument systems hadn’t even occurred to me, I found myself craving ways to physically and tactilely work with sound. 

That being said, the idea that an interface is an ideology and itself defines what is musically possible is something that really resonates with - I just think the physical input part of that system is just as important as the software side, and they should perhaps be considered holistically. 

Additionally, Magnusson mentions the importance of mapping and I found my mind wandering back to the Braitenberg vehicles. I want to try experimenting with dynamically generating mappings - whether from physical controller or GUI or internal - based on the output of the same system. I can imagine having to continuously respond to changes in input mapping itself to be a very engaging process. 



## Literature Review - Part 2
### Bibliography

Anderson, Christine. “Dynamic Networks of Sonic Interactions: An Interview with Agostino Di Scipio.” Computer Music Journal 29, no. 3 (2005): 11–28. https://doi.org/10.1162/0148926054798142. 

Bahn, Curtis, Tomie Hahn, and Dan Trueman. “Physicality and Feedback: a Focus on the Body in the Performance of Electronic Music.” Proceedings of the International Computer Music Conference, 2001, 44–51.

Berdahl, Edgar Joseph. “Applications of Feedback Control to Musical Instrument Design,” 2009. 

Bowers, John, and Sten Olof Hellström. “Simple Interfaces to Complex Sound in Improvised Music.” CHI '00 extended abstracts on Human factors in computing systems  - CHI '00, 2000. https://doi.org/10.1145/633292.633364. 

Bown, Oliver, Alice Eldridge, and Jon Mccormack. “Understanding Interaction in Contemporary Digital Music: from Instruments to Behavioural Objects.” Organised Sound 14, no. 2 (2009): 188–96. https://doi.org/10.1017/s1355771809000296. 

Camurri, A., G. Volpe, G. De Poli, and M. Leman. “Communicating Expressiveness and Affect in Multimodal Interactive Systems.” IEEE Multimedia 12, no. 1 (2005): 43–53. https://doi.org/10.1109/mmul.2005.2. 

Clarke, Eric F. Ways of Listening an Ecological Approach to the Perception of Musical Meaning. Oxford: Oxford University Press, 2005. 

Collins, N. M. “Towards Autonomous Agents for Live Computer Music: Realtime Machine Listening and Interactive Music Systems,” 2006. 

Cypess, Rebecca, and Steven Kemper. “The Anthropomorphic Analogy: Humanising Musical Machines in the Early Modern and Contemporary Eras.” Organised Sound 23, no. 2 (2018): 167–80. https://doi.org/10.1017/s1355771818000043. 

Donnarumma, Marco. “Music for the Flesh II: Informing Interactive Music Performances with the Viscerality of the Body System.” NIME, 2012. 

Driscoll, John, and Matt Rogalsky. “David Tudor's Rainforest: An Evolving Exploration of Resonance.” Leonardo Music Journal 14 (2004): 25–30. https://doi.org/10.1162/0961121043067415.

Eck, Cathy van. Between Air and Electricity: Microphones and Loudspeakers as Musical Instruments. New York, NY: Bloomsbury Academic, 2018. 

Godøy, Rolf Inge., and Marc Leman. Musical Gestures Sound, Movement, and Meaning. New York: Routledge, 2010. 

Hegarty, Paul. “Noise Music.” The Semiotic Review of Books, 2006, 1–6. 

Kahn, Douglas. Noise, Water, Meat: a History of Sound in the Arts. Cambridge, MA: MIT Press, 1999. 

Kemper, Steven, and Rebecca Cypess. “Can Musical Machines Be Expressive? Views from the Enlightenment and Today.” Leonardo 52, no. 5 (2019): 448–54. https://doi.org/10.1162/leon_a_01477. 

Knight, Arthur, and Simon Frith. “Performing Rites: On the Value of Popular Music.” American Music 16, no. 4 (1998): 485. https://doi.org/10.2307/3052293. 

LaBelle, Brandon. Sonic Agency: Sound and Emergent Forms of Resistance. Cambridge, MA: MIT Press, 2020. 

Landy, Leigh. Understanding the Art of Sound Organization. Cambridge, MA: MIT Press, 2007. 

Leman, Marc. Embodied Music Cognition and Mediation Technology. Cambridge, MA: MIT Press, 2008. 

Lähdeoja, Otso. “Composing the Context: Considerations on Materially Mediated Electronic Musicianship.” Organised Sound 23, no. 1 (2017): 61–70. https://doi.org/10.1017/s1355771817000280. 

Maes, Pieter-Jan, Marc Leman, Caroline Palmer, and Marcelo M. Wanderley. “Action-Based Effects on Music Perception.” Frontiers in Psychology 4 (January 2014). https://doi.org/10.3389/fpsyg.2013.01008.

Magnusson, Thor. “Designing Constraints: Composing and Performing with Digital Musical Systems.” Computer Music Journal 34, no. 4 (2010): 62–73. https://doi.org/10.1162/comj_a_00026. 

Oliveros, Pauline. Software for People: Collected Writings 1963-80. Kingston, NY: Pauline Oliveros Publications, 2015.

Overholt, Dan, Edgar Berdahl, and Robert Hamilton. “Advancements in Actuated Musical Instruments.” Organised Sound 16, no. 2 (2011): 154–65. https://doi.org/10.1017/s1355771811000100.

Raphael, Christopher. “Music Plus One and Machine Learning.” ICML 2010 - Proceedings, 27th International Conference on Machine Learning, 2010, 21–28.

Reybrouck, Mark, and Elvira Brattico. “Neuroplasticity beyond Sounds: Neural Adaptations Following Long-Term Musical Aesthetic Experiences.” Brain Sciences 5, no. 1 (2015): 69–91. https://doi.org/10.3390/brainsci5010069.

Rowe, Robert. Interactive Music Systems: Machine Listening and Composing. Cambridge, MA: MIT Press, 1994.

Samuels, Koichi, and Franziska Schroeder. “Performance without Barriers: Improvising with Inclusive and Accessible Digital Musical Instruments.” Contemporary Music Review 38, no. 5 (2019): 476–89. https://doi.org/10.1080/07494467.2019.1684061.

Schiavio, Andrea, and Simon Høffding. “Playing Together without Communicating? A Pre-Reflective and Enactive Account of Joint Musical Performance.” Musicae Scientiae 19, no. 4 (2015): 366–88. https://doi.org/10.1177/1029864915593333.

Scipio, Agostino Di. “Dwelling in a Field of Sonic Relationships.” Live Electronic Music, 2017, 17–45. https://doi.org/10.4324/9781315776989-2.

Scipio, Agostino Di. “‘Sound Is the Interface’: from Interactive to Ecosystemic Signal Processing.” Organised Sound 8, no. 3 (2003): 269–77. https://doi.org/10.1017/s1355771803000244.

Sterne, Jonathan. “Music as a Media Problem: Some Comments and Approaches.” Repercussions, 2014. http://www.ocf.berkeley.edu/~repercus/wpcontent/uploads/2015/1/repercussions-Vol-11–Sterne-Jonathan-Music-as-aMedia-Problem.pdf.

Tanaka, Atau. “Sensor-Based Musical Instruments and Interactive Music.” Essay. In The Oxford Handbook of Computer Music. Oxford: Oxford University Press, 2011.

Truax, Barry. “Electroacoustic Music and the Soundscape: The Inner and Outer World.” Essay. In Companion to Contemporary Musical Thought, 374–98. London: Routledge, 1992.

Voskuhl, Adelheid. Androids in the Enlightenment: Mechanics, Artisans, and Cultures of the Self. Chicago, IL: The University of Chicago Press, 2015.

Wanderley, M.m., and P. Depalle. “Gestural Control of Sound Synthesis.” Proceedings of the IEEE 92, no. 4 (2004): 632–44. https://doi.org/10.1109/jproc.2004.825882.

Waters, Simon. “Performance Ecosystems: Ecological Approaches to Musical Interaction.” EMS: Electroacoustic Music Studies Network , 2007, 1–20.

Wessel, David, and Matthew Wright. “Problems and Prospects for Intimate Musical Control of Computers.” Computer Music Journal 26, no. 3 (2002): 11–22. https://doi.org/10.1162/014892602320582945.

Xia, Guangyu. “Expressive Collaborative Music Performance via Machine Learning,” 2016.



## Journal 1

### Sketch #1: GrainSOM

Looping is an important of my practice - the trajectory of my instrumentation included a path from guitar -> guitar with pedals -> guitar with laptop processing -> computer based instrumentation. Throughout that chain, looping has been a fundamental part. With it has come the challenge of using and interacting with loops that allows them to move beyond static repetitions with maybe variations in time or pitch - how can I capture some sonic input and have dynamically transform and react over time? Some of my recent performance systems have included looping control gestures - draw some sort of shape or path with a stylus, map it out to some synthesis process, and then loop that control data. Seeing the initial Yellowtail sketch, I instantly imagined each mark as a control gesture, and it’s movement in space a possible way for having it evolve over time.


<p class="codepen" data-height="377" data-theme-id="dark" data-default-tab="js,result" data-user="kieranmaraj" data-slug-hash="YzqvMPw" data-preview="true" style="height: 377px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;" data-pen-title="GrainSOM">
  <span>See the Pen <a href="https://codepen.io/kieranmaraj/pen/YzqvMPw">
  GrainSOM</a> by Kieran Maraj (<a href="https://codepen.io/kieranmaraj">@kieranmaraj</a>)
  on <a href="https://codepen.io">CodePen</a>.</span>
</p>
<script async src="https://static.codepen.io/assets/embed/ei.js"></script>



Trying to implement this also allowed me to start exploring something that’s been on my to-do list for awhile now: audio programming outside of a visual programming environment. I used the tone.js library as a starting point, but wasn’t particularly impressed with their default granular algorithm. I ended up basing my own implementation on the one at gloo.xyz. It feels like a vastly different way of thinking about audio than in Max or pd. One difference was a clear lack of differentiation between float values and signals, though I imagine this might just be own lack of familiarity with the API. A bigger difference, however, was in the interaction with scheduling events. Starting grains and envelopes seems to always involve taking the time at a given moment, as well as calculating when it will end. I feel like there’s no clear analogy to this in Max - I wonder is this kind of thing happening under the hood and completely extracted away? Overall it was a fun little challenge to sink into. I’ve become more interested in DSP programming over time, but don’t have the strongest math background and always find approaching this world intimidating and challenging.
	While I’m satisfied with this sketch as a first attempt, there are a number of things I would change on a second iteration. The main thing I would focus on is that the motion of each mark doesn’t clearly translate into what is heard in the synthesis. In its current state, I use a SOM trained on a set of random vectors to set synthesis parameters - as a mark traverses the screen, the head of the mark samples from the SOM and updates the synthesis parameters. I thought this might be an interesting way of creating variation and unpredictability, but also allow for a smoother morphing through synthesis states. However, I think the end result feels too decoupled from the marks motion.
	A few things I would do on a second iteration:
- A mark’s x position would set it’s grain’s panning position
- A mark’s y position would set the grain’s size
- Distance from centre of the screen sets sample position
- A mark’s change in velocity modulates it’s pitch - a sort of doppler effect
- A mark’s length controls the reverb mix for a grain - longer mark resulting in more reverb, or perhaps the reverse
- A mark’s alpha sets it’s grain’s volume
- The SOM would instead modulate synthesis parameters instead of directly setting those parameters

### No Input Mixing

The prompt to reflect on no input mixing was a nice excuse to revisit the practice, as its been a year or two since I last worried I might push my monitors too hard :)

Part of the magic in no input mixing is that it feels like this dynamic, unpredictable beast, but as you spend time with it there are general mappings or shapes of mappings that are learnable, or at least knowable in the sense that in you can know that with *this set of* connections, turning up *these* sends will make *those* sliders effect *this* thing about the sound. But there’s always subtle variation. Every slight turn of a knob or push of a fader results in some connected change that you then have to react to as a performer. In a way it becomes a duet between human and machine. The mixer provides a sort of access point into a dense sonic world that must then be sculpted and shaped, and that shaping in turn effects and transforms that sonic world.

The complexity and surprises of the system, while wonderful and exciting, present an interesting challenge, though. While it's a joy to play solo, and undoubtedly works well in improvised and/or noise contexts, playing no input mixer in anything even approaching a "traditional" ensemble is difficult. The sonic aesthetics of the system are one aspect of why this might be, but I think it's actually the unpredictability and reactivity of the instrument that present a bigger challenge. I can certainly imagine NIM squeals and jitters working in a rock context, or its overwhelming bass working in dance music - but perhaps only if performing those things is repeatable? 

This leads me towards an a potential area of research. I subscribe to Christopher Small's definition of 'music' as being socially constructed - that music is not the score, it's not the vinyl record, it's not the encoded mp3 file, or even the human perception of modulated air. Small defines music as being the set of social relations that come together to make up a musical performance - everyone from the performers themselves to the audience, sound engineer to custodial staff, all are involved to varying degrees in the act of musicking. So my question then is how does the involvement of machines, particularly 'intelligent' machines (whatever that might mean), complicate that set of relations? What does it mean if a machinic instrument can only be performed with other machines, or only in particular ways, to the exclusion of human performers?





